---
title: RAGに代わる新しいアプローチ：Cache-Augmented Generation (CAG)の可能性
tags:
  - 機械学習
  - AI
  - rag
  - LLM
private: false
updated_at: '2025-01-29T14:27:58+09:00'
id: 4aec6ec2c9f799c4aa80
organization_url_name: null
slide: false
ignorePublish: false
---

## はじめに

最近、大規模言語モデル(LLM)の外部知識を活用する手法として、Retrieval-Augmented Generation (RAG)が注目を集めています。しかし、RAGにはいくつかの課題があります。今回は、RAGに代わる新しいアプローチとして提案されているCache-Augmented Generation (CAG)について解説します。

## RAGの課題

RAGには以下のような課題があります：

- 検索の遅延が発生する
- ドキュメント選択のエラーが起こりうる 
- システムの複雑性が高い

## CAGとは

CAGは、これらの課題を解決する新しいアプローチです。主な特徴は：

1. 事前に関連するリソースをLLMのコンテキストにプリロード
2. ランタイムパラメータをキャッシュ
3. 推論時に追加の検索ステップが不要

特に、限定された知識ベースを扱うケースで効果を発揮します。

## CAGの動作の仕組み

CAGは以下の3つのステップで動作します：

### 1. 外部知識のプリロード
- 関連文書をLLMの拡張コンテキストウィンドウに合わせて前処理
- キー・バリュー（KV）キャッシュを生成
- 生成されたキャッシュをディスクまたはメモリに保存

### 2. 推論処理
- ユーザーのクエリと事前計算されたKVキャッシュを使用
- 実時間の検索なしで回答を生成
- 検索遅延とエラーを防止

### 3. キャッシュのリセット
- システムのパフォーマンスを維持するための効率的なキャッシュリセット
- 新しいトークンが追加された際の迅速な再初期化

## 実験結果と性能評価

研究チームはSQuADとHotPotQAデータセットを使用して、CAGと従来のRAGシステムの性能を比較しました：
- BERTScoreによる評価で、CAGは多くのケースでRAGを上回る性能を示しました
- 特に検索エラーが発生しやすい状況での優位性が顕著
- 標準的なin-context学習と比較して、生成時間が大幅に短縮

## RAGとCAGの比較

CAGの利点：
- 検索遅延の排除
- 検索エラーの最小化
- システムの単純化

一方で、以下のような制約もあります：
- コンテキストウィンドウのサイズに制限される
- 大規模な知識ベースには不向き

## 今後の展望

- より大規模な知識ベースを処理できるLLMの発展に伴うCAGの可能性拡大
- CAGとRAGを組み合わせたハイブリッドアプローチの研究
- コンテキストの完全性と柔軟性を両立する新しい手法の開発

## まとめ

CAGは、特定のユースケース（限定された知識ベース）において、RAGに代わる効率的な選択肢となる可能性があります。今後のLLMのコンテキストウィンドウの拡大に伴い、さらなる応用が期待されます。

<br>

👉 原文は[こちら](https://arxiv.org/abs/2412.15605?utm_source=pytorchkr&ref=pytorchkr)でご確認いただけます。
